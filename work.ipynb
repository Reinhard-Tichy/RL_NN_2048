{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验说明\n",
    "\n",
    "## 作业说明\n",
    "\n",
    "### 目标：\n",
    "\n",
    "训练一个玩2048的神经网络，并得到较高的准确率。\n",
    "\n",
    "### 背景：\n",
    "\n",
    "2048是一个益智小游戏，规则为：控制所有方块向同一个方向运动，两个相同数字方块撞在一起后合并，成为他们的和。每次操作时会随机生成一个2或者4，最终得到一个“2048”的方块就算胜利。规则的直观解释：[Click to Play 2048](https://play2048.co/)\n",
    "\n",
    "本教程展示如何训练一个玩2048的神经网络模型，并评估其最终能够得到的分数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建模过程：\n",
    "\n",
    "2048游戏可以理解为一个这样的过程：\n",
    "\n",
    "<blockquote>\n",
    "    \n",
    "有一个**局面（state）**，4x4格子上的一些数字。\n",
    "    \n",
    "<img src=\"https://data.megengine.org.cn/megstudio/images/2048_demo.png\" width=256 height=256 />\n",
    "\n",
    "你可以选择做一些**动作（action）**，比如按键盘上的上/下/左/右键。\n",
    "\n",
    "你有一些**策略（policy）**，比如你觉得现在按左最好，因为这样有两个8可以合并。对于每个动作，可以用一个打分函数来决定你的策略。\n",
    "\n",
    "在按照策略做完动作之后，你会得到一个**奖励（reward）**，比如因为两个8合并，分数增加了16，这个16可以被看作是这一步的奖励。\n",
    "\n",
    "在许多步之后，游戏结束，你会得到一个**回报（return）**，即游戏的最终分数。\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "由此，我们将2048建模为一个马尔可夫决策过程，其求解可以通过各种强化学习方法来完成。在baseline中，我们使用了 [Double DQN](https://arxiv.org/abs/1509.06461)。\n",
    "\n",
    "### 任务：\n",
    "\n",
    "Q1：训练模型\n",
    "\n",
    "运行baseline，训练和评估模型。观察游戏结束时的滑动平均分数。你可以调用`print_grid`函数输出模型玩游戏的过程，以判断模型是否可以得到合理的结果。\n",
    "提供参考数据：纯随机游玩，平均分数约为570分。在baseline的训练过程中，模型最高可以达到8000分，平均为2000分左右。\n",
    "\n",
    "请你修改参数，模型结构等，使得游戏的平均分数尽可能地高。请注意：这里的平均分数指每个游戏结束**最终分数**的平均值。\n",
    "**请于q1.diff提交你的代码。**\n",
    "\n",
    "## 数据集\n",
    "\n",
    "2048游戏代码来源：[console2048](https://github.com/Mekire/console-2048/blob/master/console2048.py)\n",
    "\n",
    "## 文件存储\n",
    "实验中生成的文件可以存储于 workspace 目录中。 查看工作区文件，该目录下的变更将会持久保存。 您的所有项目将共享一个存储空间，请及时清理不必要的文件，避免加载过慢。\n",
    "\n",
    "## 实验步骤\n",
    "\n",
    "1.导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megengine as mge\n",
    "import numpy as np \n",
    "import megengine.module as M\n",
    "import megengine.functional as F\n",
    "import megengine.data.transform as T\n",
    "from random import random, randint, shuffle\n",
    "from megengine.optimizer import Adam\n",
    "from megengine.autodiff import GradManager\n",
    "from megengine import tensor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import pickle as pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2048游戏函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace.backup.game_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.定义记忆回放类并实例化\n",
    "\n",
    "在记录一次决策过程后，我们存储到该类中，并在训练时选择一部分记忆进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace.backup.perm import rpm, perm\n",
    "\n",
    "# data = rpm(5000)\n",
    "data = perm(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.定义模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace.backup.models import Net, WideNet\n",
    "\n",
    "model = WideNet()\n",
    "model_target = WideNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.定义输入转化函数，使得局面可以被输入进模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {2**i: i for i in range(1, 16)}\n",
    "table[0] = 0\n",
    "\n",
    "def make_input(grid):\n",
    "    # 每个网格对应一个16维向量, 若网格分数为2**i，则向量的第i个分量为1\n",
    "    g0 = grid\n",
    "    r = np.zeros(shape=(16, 4, 4), dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            v = g0[i, j]\n",
    "            r[table[v], i, j] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_mult_step_learning_pri: beta=0.99, epsilon=0.3\\n_mult_step_learning_pri_2: beta=0.8, epsilon=0.1\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_mult_step_learning_pri: beta=0.99, epsilon=0.3\n",
    "_mult_step_learning_pri_2: beta=0.8, epsilon=0.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.定义优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(model.parameters(), lr=5e-4)\n",
    "code_label = \"_mult_step_learning_pri_2\"\n",
    "batch_size = 512\n",
    "beta = 0.8 # multi_step weight\n",
    "epsilon = 0.1\n",
    "epochs = 10000\n",
    "dir_path = \"workspace/lhw\"\n",
    "model_path = os.path.join(dir_path, f\"{code_label}.mge\")\n",
    "loss_path = os.path.join(dir_path, f'all_loss{code_label}.npy')\n",
    "scores_path = os.path.join(dir_path, f'all_scores{code_label}.npy')\n",
    "avg_scores_path = os.path.join(dir_path, f'all_avg_scores{code_label}.npy')\n",
    "\n",
    "maxscore = 0\n",
    "avg_score = 0\n",
    "\n",
    "game = []\n",
    "all_loss = []\n",
    "all_scores = []\n",
    "all_avg_scores = []\n",
    "'''Play 32 games at the same time'''\n",
    "game = [Game() for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "修改reward, 鼓励最大值放在右下角\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  25%|██▍       | 12288/50000 [3:37:44<11:00:45,  1.05s/it, loss=2.49913, Q=142.00520, reward=107.50761, history_reward=98.66269, all_reward=206.17030, avg_score=2878.89283, max_score=8488, epsilon=0.00998, game_times=3775] "
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs*5, desc=\"epoch\") as tq:\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        '''double DQN'''\n",
    "        if epoch % 10 == 0:  # 每隔10个epoch更新一次target网络\n",
    "\n",
    "            mge.save(model, model_path)\n",
    "            model_target = mge.load(model_path)\n",
    "            np.save(loss_path, np.array(all_loss))\n",
    "            np.save(scores_path, np.array(all_scores))\n",
    "            np.save(avg_scores_path, np.array(all_avg_scores))\n",
    "            if len(all_scores) > 6000:\n",
    "                break\n",
    "\n",
    "        grid = []\n",
    "        for k in range(batch_size):\n",
    "            '''Check if the game is over'''\n",
    "            if any_possible_moves(game[k].grid) is False:  # 第k个game游戏结束\n",
    "                all_scores.append(game[k].score)\n",
    "                if avg_score == 0:\n",
    "                    avg_score = game[k].score\n",
    "                else:\n",
    "                    avg_score = avg_score * 0.99 + game[k].score * 0.01\n",
    "                all_avg_scores.append(avg_score)\n",
    "                game[k] = Game()  # 重新开始一个game\n",
    "\n",
    "            tmp = make_input(game[k].grid)\n",
    "            grid.append(tensor(tmp))  # 将所有game的grid转化为状态张量\n",
    "\n",
    "        status = F.stack(grid, 0)  # status: [b, 16, 4, 4]\n",
    "\n",
    "        '''Choose the action with the highest probability'''\n",
    "        a = F.argmax(model(status).detach(), 1)\n",
    "        a = a.numpy().copy()  # action均使用model \n",
    "        for i in range(batch_size):\n",
    "            if random.random() < epsilon:\n",
    "                a[i] = random.randint(0, 3)\n",
    "\n",
    "        if (len(all_scores) > 10000) or (epsilon > 0.01 and epoch % 5 == 0):\n",
    "            epsilon /= 1.005\n",
    "\n",
    "        s0_s = []\n",
    "        s1_s = []\n",
    "        ak_s = []\n",
    "        reward_s = []\n",
    "        done_s = []\n",
    "        history_reward_s = []\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            pre_score = game[k].score\n",
    "            prev_max = game[k].max()\n",
    "            pre_grid = game[k].grid.copy()\n",
    "            empty1 = find_empty_cell(pre_grid)\n",
    "            game[k].move(a[k])\n",
    "            after_score = game[k].score\n",
    "            after_max = game[k].max()\n",
    "            if game[k].score > maxscore:\n",
    "                maxscore = game[k].score\n",
    "            action = a[k]\n",
    "\n",
    "            '''In some situations, some actions are meaningless, try another'''\n",
    "            while (game[k].grid == pre_grid).all():\n",
    "                action = (action + 1) % 4\n",
    "                game[k].move(action)\n",
    "\n",
    "            empty2 = find_empty_cell(game[k].grid)\n",
    "            score = after_score - pre_score\n",
    "\n",
    "            reward = 0\n",
    "            if prev_max == after_max:\n",
    "                reward += math.log2(after_max) * 0.1 # 最大值奖励(只有最大值不改变时才加入?)\n",
    "            \n",
    "            reward += (empty2 - empty1) # 空格数增加奖励\n",
    "            reward += score / 128 # 总得分奖励\n",
    "\n",
    "            history_reward = game[k].reward\n",
    "            if history_reward == 0:\n",
    "                game[k].reward = reward\n",
    "            else:\n",
    "                game[k].reward = history_reward * beta + reward * (1 - beta)\n",
    "\n",
    "            done = any_possible_moves(game[k].grid) is False\n",
    "            grid = game[k].grid.copy()\n",
    "\n",
    "            '''Record to memory'''\n",
    "            '''(status, next_status, action, score, if_game_over)'''\n",
    "            s0 = tensor(make_input(pre_grid))\n",
    "            s1 = tensor(make_input(grid))\n",
    "\n",
    "            s0_s.append(s0)\n",
    "            s1_s.append(s1)\n",
    "            ak_s.append(tensor(a[k]))\n",
    "            reward_s.append(tensor(reward))\n",
    "            history_reward_s.append(tensor(history_reward))\n",
    "            done_s.append(tensor(done))\n",
    "\n",
    "        s0_t = F.stack(s0_s, 0)\n",
    "        s1_t = F.stack(s1_s, 0)\n",
    "        #ak_t = F.stack(ak_s, 0)\n",
    "        #done_t = F.stack(done_s, 0)\n",
    "        #reward_t = F.stack(reward_s, 0)\n",
    "        #history_reward_t = F.stack(history_reward_s, 0)\n",
    "        #print(ak_t.shape)\n",
    "        cur_vals = model(s0_t).detach()\n",
    "        #print(cur_vals.shape)\n",
    "        tar_vals = F.max(model_target(s1_t), axis=1).detach()\n",
    "        del s0_t, s1_t\n",
    "        #print(cur_vals.shape, tar_vals.shape)\n",
    "        #print(cur_vals.shape, tar_vals.shape, done_t.shape, reward_t.shape, history_reward_t.shape)\n",
    "        #err = F.abs(cur_vals - (tar_vals * 0.99 * (1 - done_t) + reward_t + history_reward_t))\n",
    "        #print(err.shape)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            #print(type(err[i]), err.shape)\n",
    "            #print(reward_s[i].shape)\n",
    "            cur_val = cur_vals[i][ak_s[i]]\n",
    "            # err = 0\n",
    "            err = F.abs(cur_val - (tar_vals[i] * 0.99 * (1 - done_s[i]) + reward_s[i] + history_reward_s[i])).numpy().item()\n",
    "            #print(type(err))\n",
    "            data.append((s0_s[i], s1_s[i], ak_s[i], reward_s[i], done_s[i], history_reward_s[i]), err)\n",
    "\n",
    "            '''\n",
    "            current_value = model(F.expand_dims(s0, 0))[0][a[k]]  \n",
    "            target_value = F.max(model_target(F.expand_dims(s1, 0)), axis=1)[0]\n",
    "            #print(type(reward), reward)\n",
    "            #print(type(history_reward), history_reward)\n",
    "            #print(type(current_value), current_value)\n",
    "            #print(type(target_value), target_value)\n",
    "            err = current_value.item() - (target_value.item() * 0.99 * (1-done) + reward + history_reward)\n",
    "            error = abs(err)\n",
    "\n",
    "            data.append((s0, s1, tensor(a[k]), tensor(reward), tensor(done),\n",
    "                         tensor(history_reward)), error)\n",
    "            '''\n",
    "\n",
    "        for i in range(5):\n",
    "            gm = GradManager().attach(model.parameters())\n",
    "            with gm:\n",
    "                res, idxs, weights = data.sample_batch(batch_size)\n",
    "                s0, s1, a, reward, d, history_reward = res\n",
    "                # errors = np.empty((0, batch_size), dtype=np.float64)\n",
    "\n",
    "                '''double DQN'''\n",
    "                pred_s0 = model(s0)  # (b, 4)\n",
    "                # (b, 1), target_Q 选择另一张Q值表\n",
    "                pred_s1 = F.max(model_target(s1), axis=1)\n",
    "\n",
    "                loss = 0\n",
    "                total_Q = 0\n",
    "                total_reward = 0\n",
    "                total_history_reward = 0\n",
    "                total_all_reward = 0\n",
    "                for i in range(batch_size):\n",
    "                    Q = pred_s0[i][a[i]]  # 预测值 Q(S,a)\n",
    "                    total_Q += Q\n",
    "                    total_reward += reward[i]\n",
    "                    total_history_reward += history_reward[i]\n",
    "                    tar_val = pred_s1[i].detach() * 0.99 * (1 - d[i]) + reward[i] + history_reward[i]\n",
    "                    error = F.abs(Q - tar_val)\n",
    "                    data.update(idxs[i], error.item())\n",
    "\n",
    "                    loss += float(weights[i]) * F.loss.square_loss(Q, tar_val)\n",
    "\n",
    "                loss /= batch_size\n",
    "                total_Q /= batch_size\n",
    "                total_reward = total_reward / batch_size * 128\n",
    "                total_history_reward = total_history_reward / batch_size * 128\n",
    "                total_all_reward = total_reward + total_history_reward\n",
    "                tq.set_postfix(\n",
    "                            {\n",
    "                                \"loss\": \"{0:1.5f}\".format(loss.numpy().item()),\n",
    "                                \"Q\": \"{0:1.5f}\".format(total_Q.numpy().item()),\n",
    "                                \"reward\": \"{0:1.5f}\".format(total_reward.numpy().item()),\n",
    "                                \"history_reward\": \"{0:1.5f}\".format(total_history_reward.numpy().item()),\n",
    "                                \"all_reward\": \"{0:1.5f}\".format(total_all_reward.numpy().item()),\n",
    "                                \"avg_score\":\"{0:1.5f}\".format(avg_score),\n",
    "                                \"max_score\":\"{}\".format(maxscore),\n",
    "                                \"epsilon\":\"{0:1.5f}\".format(epsilon),\n",
    "                                \"game_times\":\"{}\".format(len(all_scores)),\n",
    "                            }\n",
    "                        )\n",
    "                tq.update(1)\n",
    "                all_loss.append(loss.numpy().item())\n",
    "                gm.backward(loss)\n",
    "\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "print(\"maxscore:{}\".format(maxscore))\n",
    "print(\"avg_score:{}\".format(avg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_vals[0]\n",
    "ak_s[0].item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
